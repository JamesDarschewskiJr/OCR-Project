{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phi-3.5-Vision Model\n",
    "\n",
    "##### This model was released during the final week of this project, so this code is not nearly as refined and more specific but it follows the same structure as the Phi-3-Vision model without vLLM inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "from transformers import AutoModelForCausalLM \n",
    "from transformers import AutoProcessor \n",
    "import torch\n",
    "import gc\n",
    "import base64\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def encode_image_base64(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "    return encoded_string\n",
    "\n",
    "def run_phi35v(image_directory, output_file):\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "\n",
    "    # Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        device_map=\"cuda\", \n",
    "        trust_remote_code=True, \n",
    "        torch_dtype=\"auto\", \n",
    "        _attn_implementation='flash_attention_2'    \n",
    "    )\n",
    "\n",
    "    # For best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "    processor = AutoProcessor.from_pretrained(\n",
    "        model_id, \n",
    "        trust_remote_code=True, \n",
    "        num_crops=4\n",
    "    )\n",
    "\n",
    "    images = []\n",
    "    placeholder = \"\"\n",
    "    count = 1\n",
    "\n",
    "    image_files = sorted((f for f in os.listdir(image_directory) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))), key = natural_sort_key)\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "\n",
    "        for image_file in image_files:\n",
    "            image = image_directory + image_file\n",
    "            images.append(Image.open(image))\n",
    "            placeholder += f\"<|image_{count}|>\\n\"\n",
    "            count += 1\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"<|system|>{placeholder}ROLE: Expert image analyst, capable of extracting text from images\n",
    "                                                            TASK: Please extract all the text from the following image. The image contains various types of printed text. \n",
    "                                                            You must follow these Steps to extract and format the text: \n",
    "                                                            Step 1: Extracting\n",
    "                                                            Extract the text from the image in the order in which it is designed to be read. \n",
    "                                                            Step 2: Formatting\n",
    "                                                            Arrange the text into clean, easily readable blocks of text.<|end|>\n",
    "                                                            <|user|>Can you please extract all the text from the following image?<|end|>\\n<|assistant|>\\n\"\"\"},\n",
    "        ]\n",
    "\n",
    "        prompt = processor.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "        generation_args = { \n",
    "            \"max_new_tokens\": 1000, \n",
    "            \"temperature\": 0.0, \n",
    "            \"do_sample\": False, \n",
    "        }\n",
    "\n",
    "        generate_ids = model.generate(\n",
    "            **inputs, \n",
    "            eos_token_id=processor.tokenizer.eos_token_id, \n",
    "            **generation_args\n",
    "        )\n",
    "\n",
    "        # Remove input tokens \n",
    "        generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "        response = processor.batch_decode(generate_ids, \n",
    "            skip_special_tokens=True, \n",
    "            clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "        # print(response)\n",
    "        f.write(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e037ed845144b2182a178b5eb0a2233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darshewskijadmin@consilio.com/.cache/pypoetry/virtualenvs/llm-experimentation-IFbIb2Mw-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|image_1|>\n",
      "<|image_2|>\n",
      "<|image_3|>\n",
      "<|image_4|>\n",
      "<|image_5|>\n",
      "<|image_6|>\n",
      "<|image_7|>\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/Phi-3.5-vision-instruct\"\n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"cuda\", \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=\"auto\", \n",
    "    _attn_implementation='flash_attention_2'    \n",
    ")\n",
    "\n",
    "# For best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id, \n",
    "    trust_remote_code=True, \n",
    "    num_crops=4\n",
    ")\n",
    "\n",
    "images = []\n",
    "placeholder = \"\"\n",
    "count = 1\n",
    "\n",
    "# Path to the directory containing your local images\n",
    "image_dir = \"/home/darshewskijadmin@consilio.com/ExperimentalLLMs/LowResolutionMobyDickImages/Chapter1/\"\n",
    "\n",
    "image_files = sorted((f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))), key = natural_sort_key)\n",
    "\n",
    "for image_file in image_files:\n",
    "    image = image_dir + image_file\n",
    "    images.append(Image.open(image))\n",
    "    placeholder += f\"<|image_{count}|>\\n\"\n",
    "    count += 1\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"\"\"<|system|>{placeholder}ROLE: Expert image analyst, capable of extracting text from images\n",
    "                                                    TASK: Please extract all the text from the following image. The image contains various types of printed text. \n",
    "                                                    You must follow these Steps to extract and format the text: \n",
    "                                                    Step 1: Extracting\n",
    "                                                    Extract the text from the image in the order in which it is designed to be read. \n",
    "                                                    Step 2: Formatting\n",
    "                                                    Arrange the text into clean, easily readable blocks of text.<|end|>\n",
    "                                                    <|user|>Can you please extract all the text from the following image?<|end|>\\n<|assistant|>\\n\"\"\"},\n",
    "]\n",
    "\n",
    "prompt = processor.tokenizer.apply_chat_template(\n",
    "    messages, \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 1000, \n",
    "    \"temperature\": 0.0, \n",
    "    \"do_sample\": False, \n",
    "}\n",
    "\n",
    "generate_ids = model.generate(\n",
    "    **inputs, \n",
    "    eos_token_id=processor.tokenizer.eos_token_id, \n",
    "    **generation_args\n",
    ")\n",
    "\n",
    "# Remove input tokens \n",
    "generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "response = processor.batch_decode(generate_ids, \n",
    "    skip_special_tokens=True, \n",
    "    clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(placeholder)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/TestPhi3.5VTranscriptions/Chapter25.txt'\n",
    "run_phi35v(\"/home/darshewskijadmin@consilio.com/ExperimentalLLMs/LowResolutionMobyDickImages/Chapter25/\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.assets.image import ImageAsset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import os\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split('(\\d+)', s)]\n",
    "\n",
    "def phi3_5v(image_dir, output_file):\n",
    "    model_id = \"/data/models/Phi-3.5-vision-instruct\"\n",
    "    \n",
    "    # Clear GPU memory before initializing vLLM\n",
    "    clear_gpu_memory()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Configure the model\n",
    "    llm = LLM(\n",
    "        model=model_id,\n",
    "        trust_remote_code=True,\n",
    "        max_num_seqs=1,\n",
    "        max_model_len=32 * 1024\n",
    "    )\n",
    "\n",
    "    # Move model to GPU and wrap with DataParallel if multiple GPUs are available\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        llm = nn.DataParallel(llm)\n",
    "    llm = llm.to(device)\n",
    "\n",
    "    image_files = sorted((f for f in os.listdir(image_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))), key=natural_sort_key)\n",
    "\n",
    "    images = []\n",
    "    placeholder = \"\"\n",
    "    count = 1\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for image_file in image_files:\n",
    "            image = image_dir + image_file\n",
    "            images.append(Image.open(image))\n",
    "            placeholder += f\"<|image_{count}|>\\n\"\n",
    "            count += 1\n",
    "\n",
    "        try:\n",
    "            messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"<|system|>{placeholder}ROLE: Expert image analyst, capable of extracting text from images\n",
    "                                                            TASK: Please extract all the text from the following image. The image contains various types of printed text. \n",
    "                                                            You must follow these Steps to extract and format the text: \n",
    "                                                            Step 1: Extracting\n",
    "                                                            Extract the text from the image in the order in which it is designed to be read. \n",
    "                                                            Step 2: Formatting\n",
    "                                                            Arrange the text into clean, easily readable blocks of text.<|end|>\n",
    "                                                            <|user|>Can you please extract all the text from the following image?<|end|>\\n<|assistant|>\\n\"\"\"},\n",
    "            ]\n",
    "\n",
    "            prompt = processor.tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "            inputs = processor(prompt, images, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "            generation_args = { \n",
    "                \"max_new_tokens\": 1000, \n",
    "                \"temperature\": 0.0, \n",
    "                \"do_sample\": False, \n",
    "            }\n",
    "\n",
    "            generate_ids = model.generate(\n",
    "                **inputs, \n",
    "                eos_token_id=processor.tokenizer.eos_token_id, \n",
    "                **generation_args\n",
    "            )\n",
    "\n",
    "            # Remove input tokens \n",
    "            generate_ids = generate_ids[:, inputs['input_ids'].shape[1]:]\n",
    "            response = processor.batch_decode(generate_ids, \n",
    "                skip_special_tokens=True, \n",
    "                clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "            # print(response)\n",
    "            f.write(response)\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'CUDA out of memory' in str(e):\n",
    "                print(f\"RuntimeError: {e}\")\n",
    "                clear_gpu_memory()\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f94afb3368ee436aa3a1d362d8268ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darshewskijadmin@consilio.com/.cache/pypoetry/virtualenvs/llm-experimentation-IFbIb2Mw-py3.10/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "/home/darshewskijadmin@consilio.com/.cache/pypoetry/virtualenvs/llm-experimentation-IFbIb2Mw-py3.10/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f487b7c6bbe6420392688f7d41584ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b61dba21ef54c4398bf941afec8919b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f2870ac97340198dd4fba9a6a2b517",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd16c694b01549eb97e754fc4ffe1e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d152997d1cff445cb35587b2ecb40b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cb7f9b6bfe4871a48045a586ce3c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff13b70f5814438aa714b9a17cfca6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7581ca633ea547169162808fdbf9d5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ade61e1b5774816abf3c3da2da3be82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128f478d0cc9468bb0fc240aa400b0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4d06420ea24301bc98335b58f874b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5175a40c214c4086867b862646144a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4314335ec7e74c9fabeaf5cf9a50ee0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1d29a36c4f4df3a2b77c11e387c089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1802ed1be98a464cac9d1d2f38874897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35d1e7f9f4004cea85839d89d1e2b264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6daea67336742b0b70830cf59fcd9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd6d50b8675481097efb69e53981d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e5cb92632d46748c1f58e31ea5dd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baebc46c125548eaa9e73e99b0e0af59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dcf5ff26d041c4825b3bfc85eeaef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b74cba204594f3eac819a72dffb0157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "686b2652c921409b9c6301a2691ff3a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd6ace44a0c4e329ea1e6d6658b447c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6257c78c5474e71b1b2b92665299748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f2b4c18e6c4f2382c6fd6dcb9bfb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb6f77c5b464d90ae18d5953c9cad8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301546b1b9cd47e0aca85a4103d0aef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b031d12a52c04c64a4873e2bef52c013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e689626b4aa24298b84a12406a530748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61513385200b4a76adb11c7861068d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fffe884cf244812a652e6ed0d7a8e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a62a2db1954d0f8c8cbb3a472350b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166da4c2053d481e9daf4734113684c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2541a7210572447f8b538d0d3956d005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f959bfeb9fc5457ea8272dcbc98d8234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cd46a97383e4610879e7c8bc82cb370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a04603c4c8345ff9ef9c8982330c491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd647902dda449b955957909a904b53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868751d30d7943f3a0cfd6bf3565571e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d832293b52b4dbc8b489e1b42a529c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a97b33b636448dab24d2a74a605ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936c9ddd67ff435b888885a8ef821b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1db2b444fd41ac844b77aa8e5c3310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3974e24f7d204d449ab7ad6b064664a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461.0784368515015\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for f in os.listdir('/home/darshewskijadmin@consilio.com/ExperimentalLLMs/LowResolutionMobyDickImages/'):\n",
    "    image_directory = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/LowResolutionMobyDickImages/' + f + '/'\n",
    "    output_file = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/TestPhi3.5VTranscriptions/' + f + '.txt'\n",
    "    if os.path.isfile(output_file):\n",
    "        continue\n",
    "    else:    \n",
    "        run_phi35v(image_directory, output_file)    \n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(duration)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_project)",
   "language": "python",
   "name": "my_project_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
