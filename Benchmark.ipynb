{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from difflib import SequenceMatcher\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell below uses the spacy library to calculate the evaluation metrics (accuracy, precision, recall, F1 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model and increase max_length\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000  # Increase this value if needed\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Read text from a file.\n",
    "    \n",
    "    :param file_path: Path to the file.\n",
    "    :return: Text content of the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().strip()\n",
    "    return text\n",
    "\n",
    "def split_text(text, max_length):\n",
    "    \"\"\"\n",
    "    Split a text into chunks not exceeding max_length.\n",
    "    \n",
    "    :param text: The text to split.\n",
    "    :param max_length: Maximum length of each chunk.\n",
    "    :return: List of text chunks.\n",
    "    \"\"\"\n",
    "    return [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def align_sequences(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Align two sequences using the SequenceMatcher from difflib.\n",
    "    :param seq1: First sequence (list of tokens).\n",
    "    :param seq2: Second sequence (list of tokens).\n",
    "    :return: Two aligned sequences.\n",
    "    \"\"\"\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    aligned_seq1, aligned_seq2 = [], []\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag == 'equal':\n",
    "            aligned_seq1.extend(seq1[i1:i2])\n",
    "            aligned_seq2.extend(seq2[j1:j2])\n",
    "        elif tag == 'replace':\n",
    "            aligned_seq1.extend(seq1[i1:i2])\n",
    "            aligned_seq2.extend([''] * (i2 - i1))\n",
    "            aligned_seq1.extend([''] * (j2 - j1))\n",
    "            aligned_seq2.extend(seq2[j1:j2])\n",
    "        elif tag == 'delete':\n",
    "            aligned_seq1.extend(seq1[i1:i2])\n",
    "            aligned_seq2.extend([''] * (i2 - i1))\n",
    "        elif tag == 'insert':\n",
    "            aligned_seq1.extend([''] * (j2 - j1))\n",
    "            aligned_seq2.extend(seq2[j1:j2])\n",
    "    return aligned_seq1, aligned_seq2\n",
    "\n",
    "def evaluate_ocr(ocr_text, ground_truth_text):\n",
    "    \"\"\"\n",
    "    Evaluate OCR result against ground truth data using spaCy.\n",
    "\n",
    "    :param ocr_text: OCR generated text.\n",
    "    :param ground_truth_text: Ground truth text.\n",
    "    :return: Dictionary with evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Split texts if they exceed the max length\n",
    "    if len(ocr_text) > nlp.max_length or len(ground_truth_text) > nlp.max_length:\n",
    "        ocr_text_chunks = split_text(ocr_text, nlp.max_length)\n",
    "        ground_truth_text_chunks = split_text(ground_truth_text, nlp.max_length)\n",
    "    else:\n",
    "        ocr_text_chunks = [ocr_text]\n",
    "        ground_truth_text_chunks = [ground_truth_text]\n",
    "\n",
    "    all_aligned_ocr_tokens, all_aligned_ground_truth_tokens = [], []\n",
    "\n",
    "    for ocr_chunk, ground_truth_chunk in zip(ocr_text_chunks, ground_truth_text_chunks):\n",
    "        # Tokenize texts using spaCy\n",
    "        ocr_tokens = [token.text for token in nlp(ocr_chunk)]\n",
    "        ground_truth_tokens = [token.text for token in nlp(ground_truth_chunk)]\n",
    "\n",
    "        # Align sequences\n",
    "        aligned_ocr_tokens, aligned_ground_truth_tokens = align_sequences(ocr_tokens, ground_truth_tokens)\n",
    "        all_aligned_ocr_tokens.extend(aligned_ocr_tokens)\n",
    "        all_aligned_ground_truth_tokens.extend(aligned_ground_truth_tokens)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(all_aligned_ground_truth_tokens, all_aligned_ocr_tokens)\n",
    "    precision = precision_score(all_aligned_ground_truth_tokens, all_aligned_ocr_tokens, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_aligned_ground_truth_tokens, all_aligned_ocr_tokens, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_aligned_ground_truth_tokens, all_aligned_ocr_tokens, average='weighted', zero_division=0)\n",
    "\n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def process_directory(ocr_dir, ground_truth_dir, output_csv):\n",
    "    \"\"\"\n",
    "    Process a directory of OCR and ground truth texts and save the evaluation results to a CSV file.\n",
    "\n",
    "    :param ocr_dir: Directory containing OCRed texts.\n",
    "    :param ground_truth_dir: Directory containing ground truth texts.\n",
    "    :param output_csv: Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for filename in os.listdir(ocr_dir):\n",
    "        ocr_file_path = os.path.join(ocr_dir, filename)\n",
    "        ground_truth_file_path = os.path.join(ground_truth_dir, filename)\n",
    "\n",
    "        if os.path.isfile(ocr_file_path) and os.path.isfile(ground_truth_file_path):\n",
    "            ocr_text = read_text_from_file(ocr_file_path)\n",
    "            ground_truth_text = read_text_from_file(ground_truth_file_path)\n",
    "            \n",
    "            eval_results = evaluate_ocr(ocr_text, ground_truth_text)\n",
    "            eval_results['Filename'] = filename\n",
    "            \n",
    "            results.append(eval_results)\n",
    "        else:\n",
    "            print(f\"Missing ground truth file for {filename}\")\n",
    "\n",
    "    # Convert results to a DataFrame and save to CSV\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_csv(output_csv, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories containing OCR and ground truth texts\n",
    "    ocr_directory = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/TestPhi3.5VTranscriptions/'\n",
    "    ground_truth_directory = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/MobyDickTranscriptions/'\n",
    "    output_csv_file = 'Phi35_Evaluation.csv'\n",
    "\n",
    "    # Process the directory and save results to CSV\n",
    "    process_directory(ocr_directory, ground_truth_directory, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell uses the jiwer library to calculate the character error rate and word error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to Phi_Error_Rate.csv\n"
     ]
    }
   ],
   "source": [
    "ground_truth = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/MobyDickTranscriptions/'\n",
    "ocr = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/TestPhi3VTranscriptions/'\n",
    "\n",
    "ground_truth_dir = sorted(os.listdir(ground_truth))\n",
    "ocr_dir = sorted(os.listdir(ocr))\n",
    "\n",
    "if len(ground_truth_dir) != len(ocr_dir):\n",
    "    raise ValueError(\"Directories do not contian the same number of files\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for file1, file2 in zip(ground_truth_dir, ocr_dir):\n",
    "\n",
    "    with open(os.path.join(ground_truth, file1), 'r', encoding = 'utf-8') as f1:\n",
    "        text1 = f1.read()\n",
    "    \n",
    "    with open(os.path.join(ocr, file2), 'r', encoding = 'utf-8') as f2:\n",
    "        text2 = f2.read()\n",
    "\n",
    "    word_error_rate = jiwer.wer(text1, text2)\n",
    "    char_error_rate = jiwer.cer(text1, text2)\n",
    "\n",
    "    if file1 == file2:\n",
    "        results.append({\n",
    "            \"Filename\": file1,\n",
    "            \"CER\": char_error_rate,\n",
    "            \"WER\": word_error_rate\n",
    "        })\n",
    "    else:\n",
    "        results.append({\n",
    "            \"Ground Truth\": file1,\n",
    "            \"OCR\": file2,\n",
    "            \"CER\": char_error_rate,\n",
    "            \"WER\": word_error_rate\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "output_csv_path = \"Phi_Error_Rate.csv\"\n",
    "df.to_csv(output_csv_path, index = False)\n",
    "\n",
    "print(f\"Results saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This cell was only used for calculating the evaluation metrics for Phi 3.5 Vision and merge that DataFrame with the page counts for each chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from difflib import SequenceMatcher\n",
    "import spacy\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load spaCy model and increase max_length\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1500000  # Increase this value if needed\n",
    "\n",
    "def benchmark_ocr_files(ocr_dir, ground_truth_dir):\n",
    "    \"\"\"\n",
    "    Benchmark OCRed text files against ground truth texts.\n",
    "    \n",
    "    :param ocr_dir: Directory containing OCRed text files.\n",
    "    :param ground_truth_dir: Directory containing ground truth text files.\n",
    "    :return: DataFrame with evaluation metrics.\n",
    "    \"\"\"\n",
    "    ground_truth_files = os.listdir(ground_truth_dir)\n",
    "    results = []\n",
    "\n",
    "    for ocr_file in os.listdir(ocr_dir):\n",
    "        ocr_file_path = os.path.join(ocr_dir, ocr_file)\n",
    "        if os.path.isfile(ocr_file_path):\n",
    "            ocr_text = read_text_from_file(ocr_file_path)\n",
    "            best_match = soft_match(ocr_file, ground_truth_files)\n",
    "            \n",
    "            if best_match:\n",
    "                ground_truth_file_path = os.path.join(ground_truth_dir, best_match)\n",
    "                ground_truth_text = read_text_from_file(ground_truth_file_path)\n",
    "                eval_results = evaluate_ocr(ocr_text, ground_truth_text)\n",
    "                eval_results['OCR File'] = ocr_file\n",
    "                eval_results['Ground Truth File'] = best_match\n",
    "                results.append(eval_results)\n",
    "            else:\n",
    "                print(f\"No matching ground truth file found for {ocr_file}\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def count_pages_and_match(pages_dir, ground_truth_dir):\n",
    "    \"\"\"\n",
    "    Count the number of pages in subfolders and match them to ground truth text files.\n",
    "    \n",
    "    :param pages_dir: Directory containing subfolders with pages.\n",
    "    :param ground_truth_dir: Directory containing ground truth text files.\n",
    "    :return: DataFrame with page counts and matches.\n",
    "    \"\"\"\n",
    "    ground_truth_files = os.listdir(ground_truth_dir)\n",
    "    subfolder_counts = []\n",
    "\n",
    "    for subfolder in os.listdir(pages_dir):\n",
    "        subfolder_path = os.path.join(pages_dir, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            file_count = len([name for name in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, name))])\n",
    "            best_match = soft_match(subfolder, ground_truth_files)\n",
    "            subfolder_counts.append({\n",
    "                'Subfolder': subfolder,\n",
    "                'Number of Pages': file_count,\n",
    "                'Ground Truth File': best_match\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(subfolder_counts)\n",
    "\n",
    "def merge_dataframes(pages_df, ocr_df, output_csv):\n",
    "    \"\"\"\n",
    "    Merge DataFrames containing page counts and OCR evaluation metrics.\n",
    "    \n",
    "    :param pages_df: DataFrame with page counts and matches.\n",
    "    :param ocr_df: DataFrame with OCR evaluation metrics.\n",
    "    :param output_csv: Path to the final merged CSV file.\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(pages_df, ocr_df, on='Ground Truth File', how='inner')\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "\n",
    "def soft_match(subfolder_name, ground_truth_files):\n",
    "    \"\"\"\n",
    "    Perform a soft match between a subfolder name and ground truth file names.\n",
    "    \n",
    "    :param subfolder_name: Name of the subfolder.\n",
    "    :param ground_truth_files: List of ground truth file names.\n",
    "    :return: Best matching ground truth file name.\n",
    "    \"\"\"\n",
    "    best_match = None\n",
    "    highest_ratio = 0\n",
    "    for gt_file in ground_truth_files:\n",
    "        ratio = SequenceMatcher(None, subfolder_name, gt_file).ratio()\n",
    "        if ratio > highest_ratio:\n",
    "            highest_ratio = ratio\n",
    "            best_match = gt_file\n",
    "    return best_match\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Read text from a file.\n",
    "    \n",
    "    :param file_path: Path to the file.\n",
    "    :return: Text content of the file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().strip()\n",
    "    return text\n",
    "\n",
    "def evaluate_ocr(ocr_text, ground_truth_text):\n",
    "    \"\"\"\n",
    "    Evaluate OCR result against ground truth data using spaCy.\n",
    "    \n",
    "    :param ocr_text: OCR generated text.\n",
    "    :param ground_truth_text: Ground truth text.\n",
    "    :return: Dictionary with evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Tokenize and evaluate\n",
    "    ocr_tokens = [token.text for token in nlp(ocr_text)]\n",
    "    ground_truth_tokens = [token.text for token in nlp(ground_truth_text)]\n",
    "\n",
    "    # Align sequences\n",
    "    aligned_ocr_tokens, aligned_ground_truth_tokens = align_sequences(ocr_tokens, ground_truth_tokens)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy = accuracy_score(aligned_ground_truth_tokens, aligned_ocr_tokens)\n",
    "    precision = precision_score(aligned_ground_truth_tokens, aligned_ocr_tokens, average='weighted', zero_division=0)\n",
    "    recall = recall_score(aligned_ground_truth_tokens, aligned_ocr_tokens, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(aligned_ground_truth_tokens, aligned_ocr_tokens, average='weighted', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def align_sequences(seq1, seq2):\n",
    "    \"\"\"\n",
    "    Align two sequences using the SequenceMatcher from difflib.\n",
    "    \n",
    "    :param seq1: First sequence (list of tokens).\n",
    "    :param seq2: Second sequence (list of tokens).\n",
    "    :return: Two aligned sequences.\n",
    "    \"\"\"\n",
    "    matcher = SequenceMatcher(None, seq1, seq2)\n",
    "    aligned_seq1, aligned_seq2 = [], []\n",
    "    for tag, i1, i2, j1, j2 in matcher.get_opcodes():\n",
    "        if tag == 'equal':\n",
    "            aligned_seq1.extend(seq1[i1:i2])\n",
    "            aligned_seq2.extend(seq2[j1:j2])\n",
    "        elif tag == 'replace':\n",
    "            aligned_seq1.extend(seq1[i1:i2])\n",
    "            aligned_seq2.extend([''] * (i2 - i1))\n",
    "            aligned_seq1.extend([''] * (j2 - j1))\n",
    "            aligned_seq2.extend(seq2[j1:j2])\n",
    "        elif tag == 'delete':\n",
    "            aligned_seq1.extend(seq1[i1:i2])\n",
    "            aligned_seq2.extend([''] * (i2 - i1))\n",
    "        elif tag == 'insert':\n",
    "            aligned_seq1.extend([''] * (j2 - j1))\n",
    "            aligned_seq2.extend(seq2[j1:j2])\n",
    "    return aligned_seq1, aligned_seq2\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Directories containing pages, OCR transcriptions, and ground truth texts\n",
    "    pages_directory = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/LowResolutionMobyDickImages/'\n",
    "    ocr_directory = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/TestPhi3.5VTranscriptions/'\n",
    "    ground_truth_directory = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/MobyDickTranscriptions/'\n",
    "    output_csv_file = '/home/darshewskijadmin@consilio.com/ExperimentalLLMs/Data/final_merged_results.csv'\n",
    "\n",
    "    # Benchmark OCR files against ground truth texts\n",
    "    ocr_df = benchmark_ocr_files(ocr_directory, ground_truth_directory)\n",
    "\n",
    "    # Count pages in subfolders and match to ground truth texts\n",
    "    pages_df = count_pages_and_match(pages_directory, ground_truth_directory)\n",
    "\n",
    "    # Merge the DataFrames and save the final output CSV\n",
    "    merge_dataframes(pages_df, ocr_df, output_csv_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
